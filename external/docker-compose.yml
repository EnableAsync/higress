version: '3.7'
services:
  envoy:
    # image: higress-registry.cn-hangzhou.cr.aliyuncs.com/higress/gateway:v1.4.0-rc.1
    image: higress-registry.cn-hangzhou.cr.aliyuncs.com/higress/gateway:1.4.2
    entrypoint: /usr/local/bin/envoy
    # 注意这里对wasm开启了debug级别日志，正式部署时则默认info级别
    command: -c /etc/envoy/envoy.yaml --component-log-level wasm:debug
    depends_on:
    # - httpbin
    - redis
    # - weaviate
    # - chroma
    # - es
    networks:
    - wasmtest
    ports:
    - "10000:10000"
    - "9901:9901"
    volumes:
    - ./envoy.yaml:/etc/envoy/envoy.yaml
    # 注意默认没有这两个 wasm 的时候，docker 会创建文件夹，这样会出错，需要有 wasm 文件之后 down 然后重新 up
    - ./ai-cache.wasm:/etc/envoy/ai-cache.wasm
    - ./ai-proxy.wasm:/etc/envoy/ai-proxy.wasm

  # chroma:
  #   image: chromadb/chroma
  #   ports:
  #     - "8001:8000"
  #   volumes:
  #     - chroma-data:/chroma/chroma

  redis:
    image: redis:latest
    networks:
    - wasmtest
    ports:
    - "6379:6379"

  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9001:9001"
      - "9000:9000"
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  milvus:
    image: milvusdb/milvus:v2.4.13-hotfix
    command: ["milvus", "run", "standalone"]
    security_opt:
    - seccomp:unconfined
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"

  # vald:
  #   image: vdaas/vald-agent-ngt
  #   ports:
  #     - "8081:8081"
  #   volumes:
  #     - ./vald:/etc/server
  #     - /etc/passwd:/etc/passwd:ro
  #     - /etc/group:/etc/group:ro
  #   user: "${UID}:${GID}"

  # qdrant:
  #   image: qdrant/qdrant
  #   networks:
  #     - wasmtest
  #   ports:
  #     - "6333:6333"
  #   volumes:
  #     - qdrant-data:/qdrant/storage

  # httpbin:
  #   image: kennethreitz/httpbin:latest
  #   networks:
  #   - wasmtest
  #   ports:
  #   - "12345:80"

  # es:
  #   image: elasticsearch:8.15.0
  #   environment:
  #     - "TZ=Asia/Shanghai"
  #     - "discovery.type=single-node"
  #     - "xpack.security.http.ssl.enabled=false"
  #     - "xpack.license.self_generated.type=trial"
  #     - "ELASTIC_PASSWORD=123456"
  #   ports:
  #     - "9200:9200"
  #     - "9300:9300"
  #   networks:
  #     - wasmtest

  # kibana:
  #   image: docker.elastic.co/kibana/kibana:8.15.0
  #   environment:
  #     - "TZ=Asia/Shanghai"
  #     - "ELASTICSEARCH_HOSTS=http://es:9200"
  #     - "ELASTICSEARCH_URL=http://es:9200"
  #     - "ELASTICSEARCH_USERNAME=kibana_system"
  #     - "ELASTICSEARCH_PASSWORD=123456"
  #     - "xpack.security.enabled=false"
  #     - "xpack.license.self_generated.type=trial"
  #   ports:
  #     - "5601:5601"
  #   networks:
  #     - wasmtest
  #   depends_on:
  #     - es

  # lobechat:
  #   # docker hub 如果访问不了，可以改用这个地址：registry.cn-hangzhou.aliyuncs.com/2456868764/lobe-chat:v1.1.3
  #   image: lobehub/lobe-chat
  #   environment:
  #     - CODE=admin
  #     - OPENAI_API_KEY=unused
  #     - OPENAI_PROXY_URL=http://envoy:10000/v1
  #   networks:
  #     - wasmtest
  #   ports:
  #     - "3210:3210/tcp"

  # weaviate:
  #   command:
  #   - --host
  #   - 0.0.0.0
  #   - --port
  #   - '8080'
  #   - --scheme
  #   - http
  #   # 高于 1.24.x 的版本，单节点部署有问题
  #   image: cr.weaviate.io/semitechnologies/weaviate:1.24.1
  #   ports:
  #   - 8081:8080
  #   - 50051:50051
  #   volumes:
  #   - weaviate_data:/var/lib/weaviate
  #   restart: on-failure:0
  #   networks:
  #   - wasmtest
  #   environment:
  #     QUERY_DEFAULTS_LIMIT: 25
  #     AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
  #     PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
  #     DEFAULT_VECTORIZER_MODULE: 'none'
  #     ENABLE_API_BASED_MODULES: 'true'
  #     CLUSTER_HOSTNAME: 'node1'

  # t2v-transformers:  # Set the name of the inference container
  #   image: cr.weaviate.io/semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1
  #   environment:
  #     ENABLE_CUDA: 0  # Set to 1 to enable
volumes:
  weaviate_data: {}
  chroma-data:
    driver: local

networks:
  wasmtest: {}